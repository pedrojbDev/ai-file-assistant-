import os
import tempfile
import streamlit as st
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain

# Configurar a p치gina do Streamlit
st.set_page_config(page_title="Perguntas sobre Arquivos", layout="wide")

# Verificar a API Key da OpenAI
try:
    OPENAI_API_KEY = st.secrets["openai"]["api_key"]
except KeyError:
    st.error("A chave da API da OpenAI n칚o foi configurada. Verifique os 'secrets'.")
    st.stop()

# Inicializar modelos e embeddings
llm = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)


def process_file(uploaded_file):
    """
    Processa um arquivo carregado (PDF ou texto) e retorna os documentos extra칤dos.
    """
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as temp_file:
        temp_file.write(uploaded_file.read())
        temp_file.flush()

        try:
            if ext == ".pdf":
                loader = PyPDFLoader(temp_file.name)
            elif ext == ".txt":
                loader = TextLoader(temp_file.name)
            else:
                raise ValueError("Formato de arquivo n칚o suportado. Use PDF ou TXT.")
            documents = loader.load()
        finally:
            os.remove(temp_file.name)  # Excluir o arquivo tempor치rio ap칩s o uso
    return documents


def create_vectorstore(documents):
    """
    Cria e retorna um 칤ndice vetorial a partir dos documentos fornecidos.
    """
    try:
        return FAISS.from_documents(documents, embeddings)
    except Exception as e:
        raise RuntimeError(f"Erro ao criar 칤ndice vetorial: {e}")


def initialize_retriever():
    """
    Inicializa o retriever na sess칚o, caso ainda n칚o exista.
    """
    if "retriever" not in st.session_state:
        st.session_state["retriever"] = None
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []


def main():
    st.title("Sistema de Perguntas sobre Arquivos 游늯游뱄")
    st.write("Carregue um arquivo (PDF ou TXT) e fa칞a perguntas sobre o conte칰do.")

    initialize_retriever()

    # Upload do arquivo
    uploaded_file = st.file_uploader("Fa칞a upload de um arquivo", type=["txt", "pdf"])
    if not uploaded_file:
        st.info("Por favor, fa칞a upload de um arquivo para come칞ar.")
        return

    if st.button("Processar Arquivo"):
        try:
            st.write("Processando o arquivo...")
            documents = process_file(uploaded_file)
            st.success("Arquivo carregado com sucesso!")
            st.write("Criando 칤ndice vetorial...")
            vectorstore = create_vectorstore(documents)
            st.session_state["retriever"] = vectorstore.as_retriever()
            st.success("칈ndice vetorial criado com sucesso!")
        except Exception as e:
            st.error(f"Erro ao processar o arquivo: {e}")
            return

    # Verificar se o 칤ndice vetorial foi criado
    if not st.session_state["retriever"]:
        st.warning("Fa칞a upload de um arquivo e processe-o para come칞ar.")
        return

    # Entrada de perguntas do usu치rio
    st.write("### Fa칞a perguntas sobre o arquivo")
    user_question = st.text_input("Digite sua pergunta aqui:")

    if user_question:
        with st.spinner("Pensando..."):
            try:
                chain = ConversationalRetrievalChain.from_llm(llm, st.session_state["retriever"])

                # Limitar hist칩rico de conversa para economizar tokens
                MAX_HISTORY_LENGTH = 5
                if len(st.session_state["chat_history"]) > MAX_HISTORY_LENGTH:
                    st.session_state["chat_history"] = st.session_state["chat_history"][-MAX_HISTORY_LENGTH:]

                response = chain.run({"question": user_question, "chat_history": st.session_state["chat_history"]})

                # Atualizar hist칩rico de conversa
                st.session_state["chat_history"].append((user_question, response))

                # Exibir resposta
                st.write("**Resposta:**")
                st.write(response)
            except Exception as e:
                st.error(f"Erro ao processar sua pergunta: {e}")


# Executa a aplica칞칚o
if __name__ == "__main__":
    main()
