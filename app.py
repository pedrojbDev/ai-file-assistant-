import streamlit as st
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
import tempfile
import os

# Configurar a p√°gina do Streamlit
st.set_page_config(page_title="Perguntas sobre Arquivos", layout="wide")

# Configurar a API da OpenAI
try:
    OPENAI_API_KEY = st.secrets["openai"]["api_key"]
except Exception as e:
    st.error("Chave da API da OpenAI n√£o encontrada. Verifique o arquivo secrets.toml.")
    st.stop()

# Inicializar o modelo GPT e os embeddings
llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

# Fun√ß√£o para carregar e processar o arquivo
def process_file(file):
    ext = os.path.splitext(file.name)[1].lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as temp_file:
        temp_file.write(file.read())
        temp_file.flush()
        if ext == ".pdf":
            loader = PyPDFLoader(temp_file.name)
        elif ext == ".txt":
            loader = TextLoader(temp_file.name)
        else:
            raise ValueError("Tipo de arquivo n√£o suportado. Use PDF ou TXT.")
    return loader.load()

# Interface Streamlit
st.title("Sistema de Perguntas sobre Arquivos üìÑü§ñ")
st.write("Carregue um arquivo e fa√ßa perguntas sobre o conte√∫do.")

# Carregar o arquivo
uploaded_file = st.file_uploader("Fa√ßa upload de um arquivo (PDF ou texto)", type=["txt", "pdf"])

if uploaded_file:
    try:
        st.write("Processando o arquivo...")
        documents = process_file(uploaded_file)
        st.success("Arquivo carregado com sucesso!")
    except Exception as e:
        st.error(f"Erro ao processar o arquivo: {e}")
        st.stop()
    
    # Criar vetor de √≠ndice e recuperador
    with st.spinner("Criando √≠ndice para o arquivo..."):
        try:
            vectorstore = FAISS.from_documents(documents, embeddings)
        except Exception as e:
            st.error(f"Erro ao criar √≠ndice vetorial: {e}")
            st.stop()
    
    # Inicializar hist√≥rico de conversa
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []

    # Criar sess√£o de perguntas
    st.write("### Fa√ßa perguntas sobre o arquivo")
    user_question = st.text_input("Digite sua pergunta aqui:")

    if user_question:
        with st.spinner("Pensando..."):
            try:
                # Configurar o chain para busca e respostas
                retriever = vectorstore.as_retriever()
                chain = ConversationalRetrievalChain.from_llm(llm, retriever)
                
                # Adicionar hist√≥rico ao chain
                response = chain.run({"question": user_question, "chat_history": st.session_state["chat_history"]})
                
                # Atualizar hist√≥rico
                st.session_state["chat_history"].append((user_question, response))

                # Exibir resposta
                st.write("**Resposta:**")
                st.write(response)
            except Exception as e:
                st.error(f"Erro ao processar sua pergunta: {e}")
